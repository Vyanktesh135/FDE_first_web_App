{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a14fe1",
   "metadata": {},
   "source": [
    "# ðŸ“š  LLM-as-judge Evaluation\n",
    "\n",
    "LLM-as-a-Judge is a method where a Large Language Model is used not to generate answers, but to evaluate them. Instead of humans manually reviewing responses, the LLM acts like an automatic evaluator, scoring or critiquing outputs based on predefined criteria.\n",
    "\n",
    "### ðŸ“Œ Key Ideas\n",
    "\n",
    "- Role reversal: The LLM is not producing contentâ€”it is judging content produced by another model or system.\n",
    "\n",
    "- Objective evaluation: It scores responses for correctness, relevance, completeness, safety, or adherence to instructions.\n",
    "\n",
    "- Prompt-based rules: The evaluation criteria are written in a judge prompt, turning the model into a consistent reviewer.\n",
    "\n",
    "### ðŸ§  Why Use LLM-as-a-Judge\n",
    "\n",
    "- Removes the need for manual evaluation at scale\n",
    "\n",
    "- Provides consistent judgments compared to subjective human reviewers\n",
    "\n",
    "- Works well for tasks like RAG evaluation, code correctness, summarization quality, and safety checks\n",
    "\n",
    "### âš™ï¸ How It Works\n",
    "\n",
    "Provide the judge LLM with:\n",
    "\n",
    "- The prompt/question\n",
    "\n",
    "- The model response you want to evaluate\n",
    "\n",
    "- Ground truth (optional)\n",
    "\n",
    "- The judge LLM analyzes the answer using evaluation instructions.\n",
    "\n",
    "- It outputs structured feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6120d6-b7ca-45c4-b0f9-1e94a763506a",
   "metadata": {},
   "source": [
    "### ðŸ” Use Cases\n",
    "| Area                                 | What It Judges                          |\n",
    "| ------------------------------------ | --------------------------------------- |\n",
    "| Retrieval Augmented Generation (RAG) | Faithfulness, relevance, hallucinations |\n",
    "| Code Generation                      | Does generated code solve the problem?  |\n",
    "| Summarization                        | Coverage, factual accuracy              |\n",
    "| Chatbots                             | Safety, tone, harmful content           |\n",
    "| Education                            | Automated grading of open text answers  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90e786-730b-4f76-9e51-6e361182bd15",
   "metadata": {},
   "source": [
    "This notebook implement LLM-as-judge in pure Python:\n",
    "\n",
    "- Dataset: list of prompts (and optional reference answers)\n",
    "\n",
    "- Target model call:\n",
    "   prompt â†’ target_model â†’ candidate_answer\n",
    "\n",
    "- Judge model call (LLM-as-judge):\n",
    "\n",
    "- prompt + candidate_answer + (optional reference) â†’ judge_model â†’ JSON {score, label, reason}\n",
    "\n",
    "- Collect results into a list / DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726fdae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4bcc1-016e-47ef-b492-7bc25936b484",
   "metadata": {},
   "source": [
    "### 2. Helper to call completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0349c847-7d44-4edf-a194-b4329f6ba0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def openai_chat(\n",
    "    model_id: str,\n",
    "    user_text: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    temperature: float = 0.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Call an OpenAI chat model and return the text output.\n",
    "    Works with GPT-4o, GPT-4o-mini, GPT-5-series, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    # Extract plain text from response\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474e062-48db-4b9b-b134-6da2191e046b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd645c98-7db9-4515-b6a7-ad7b53316e81",
   "metadata": {},
   "source": [
    "### 3. Judge prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cda911d-43aa-4c71-b6e9-7d7a397b3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert evaluator for LLM outputs.\n",
    "You strictly follow the scoring rubric and ALWAYS respond in valid JSON only.\n",
    "Do not include any extra commentary outside of the JSON.\n",
    "\"\"\"\n",
    "\n",
    "def build_judge_user_prompt(\n",
    "    instruction: str,\n",
    "    model_response: str,\n",
    "    reference_answer: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build the user message given to the judge model.\n",
    "    The judge will score from 1 to 5 and explain briefly.\n",
    "    \"\"\"\n",
    "    ref_section = (\n",
    "        f\"\\n[Reference Answer]\\n{reference_answer}\\n\"\n",
    "        if reference_answer\n",
    "        else \"\\n[Reference Answer]\\nNone provided.\\n\"\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "You are grading a model response.\n",
    "\n",
    "[Instruction / User Prompt]\n",
    "{instruction}\n",
    "\n",
    "[Model Response]\n",
    "{model_response}\n",
    "{ref_section}\n",
    "\n",
    "Scoring rubric (1â€“5):\n",
    "- 1 = Completely wrong, irrelevant, or nonsensical.\n",
    "- 2 = Major issues; partially answers but with serious problems.\n",
    "- 3 = Acceptable but incomplete or with some issues.\n",
    "- 4 = Good answer with minor issues.\n",
    "- 5 = Excellent, complete, correct, and well-explained.\n",
    "\n",
    "Return ONLY valid JSON in this exact schema:\n",
    "{{\n",
    "  \"score\": <number between 1 and 5>,\n",
    "  \"label\": \"<one of: poor, fair, good, very_good, excellent>\",\n",
    "  \"reason\": \"<short explanation of why you gave this score>\"\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a189d9-bae1-43d4-be38-f12c11e0150b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f2edced-284f-4abc-8798-fc5b38ccbfd2",
   "metadata": {},
   "source": [
    "### 4. Judge call + JSON parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43a7be4-e2dd-4370-8ffc-fe35b6e6b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def judge_response(\n",
    "    judge_model_id: str,\n",
    "    instruction: str,\n",
    "    model_response: str,\n",
    "    reference_answer: Optional[str] = None,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Call the judge model and parse its JSON grading output.\n",
    "    \"\"\"\n",
    "    user_prompt = build_judge_user_prompt(\n",
    "        instruction=instruction,\n",
    "        model_response=model_response,\n",
    "        reference_answer=reference_answer,\n",
    "    )\n",
    "\n",
    "    raw_output = openai_chat(\n",
    "        model_id=judge_model_id,\n",
    "        user_text=user_prompt,\n",
    "        system_prompt=JUDGE_SYSTEM_PROMPT,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # Try to parse JSON robustly (in case model adds whitespace, etc.)\n",
    "    def extract_json(text: str) -> Dict:\n",
    "        # naive but often enough: find first '{' and last '}'.\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "        if start == -1 or end == -1:\n",
    "            raise ValueError(f\"Judge output is not valid JSON:\\n{text}\")\n",
    "        json_str = text[start : end + 1]\n",
    "        return json.loads(json_str)\n",
    "\n",
    "    result = extract_json(raw_output)\n",
    "\n",
    "    # Basic sanity checks\n",
    "    if \"score\" not in result or \"reason\" not in result:\n",
    "        raise ValueError(f\"JSON missing required keys: {result}\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b904d4-2367-4b9d-afda-e54f381c7131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5614ab02-134e-4825-9f8c-f13e3d22aa61",
   "metadata": {},
   "source": [
    "### 5. (Optional) target model for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8664c00f-8c80-4ef1-99e0-df33e775550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_candidate(\n",
    "    target_model_id: str,\n",
    "    instruction: str,\n",
    "    system_prompt: str = \"You are a helpful assistant.\",\n",
    ") -> str:\n",
    "    return openai_chat(\n",
    "        model_id=target_model_id,\n",
    "        user_text=instruction,\n",
    "        system_prompt=system_prompt,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f317bd-a843-4088-82ee-a44d3c0c3d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fc8ac7f-1f0e-4014-b442-87bfbe3adf0a",
   "metadata": {},
   "source": [
    "### 6. Example dataset in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75a289d5-6c36-4434-bbd9-a6737459d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: List[Dict] = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"prompt\": \"Explain what a vector database does in two simple sentences.\",\n",
    "        \"reference\": \"A vector database stores embeddings that represent text, images, or other data in high-dimensional vectors. It enables fast similarity search to find semantically related items.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"prompt\": \"List three benefits of using LLM-as-a-judge for model evaluation.\",\n",
    "        \"reference\": None,  # judge decides correctness based on reasoning quality\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"prompt\": \"Convert the sentence 'The model failed three times' into passive voice.\",\n",
    "        \"reference\": \"Three failures were encountered by the model.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"prompt\": \"Summarize the following in one sentence: 'Reinforcement learning agents learn actions through rewards and penalties and improve performance over time.'\",\n",
    "        \"reference\": \"Reinforcement learning agents optimize decisions by using feedback from rewards and penalties.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"prompt\": \"Give me 2 ways to reduce hallucinations in a RAG system.\",\n",
    "        \"reference\": \"Use citation-based generation and restrict model answers to retrieved documents.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"prompt\": \"Is this statement true or false? 'FAISS is a text generation model.' Justify your answer.\",\n",
    "        \"reference\": \"False. FAISS is a vector similarity search library, not a text generation model.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"prompt\": \"Write a single SQL query to fetch all employees earning more than 50,000.\",\n",
    "        \"reference\": \"SELECT * FROM employees WHERE salary > 50000;\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"prompt\": \"Provide a polite refusal to this request: 'Hack into my colleagueâ€™s email.'\",\n",
    "        \"reference\": \"I can't help with hacking or accessing private accounts, but I can suggest legal ways to contact your colleague.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"prompt\": \"Explain 'prompt chaining' in less than 25 words.\",\n",
    "        \"reference\": \"Prompt chaining breaks complex tasks into multiple linked prompts where each output becomes input for the next step.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"prompt\": \"Suggest three test metrics to evaluate a generative AI model.\",\n",
    "        \"reference\": None,  # judge model evaluates completeness + correctness\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4a52a-8b16-44d6-8480-a58e5bab527b",
   "metadata": {},
   "source": [
    "### 7. Configuration: model IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "070575d1-65a6-4ead-93b8-90b1f638c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Use GPT-4o / GPT-4o-mini / GPT-5 models as applicable)\n",
    "\n",
    "TARGET_MODEL_ID = \"gpt-4o-mini\"     # fast, inexpensive model under test\n",
    "JUDGE_MODEL_ID  = \"gpt-4o\"          # stronger model used as an evaluator (LLM-as-a-judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f032d-30bb-4579-983d-9640824adf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e512df65-cd7c-4bd0-9d89-bc36d4cdb0b5",
   "metadata": {},
   "source": [
    "### 8. Full evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1718d4d6-dbae-448a-937f-e57a55000707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating item 1 ===\n",
      "Prompt: Explain what a vector database does in two simple sentences.\n",
      "Candidate response:\n",
      " A vector database stores and manages data in the form of high-dimensional vectors, which represent various types of information, such as text, images, or audio. It enables efficient similarity search and retrieval of data based on the proximity of these vectors in a multi-dimensional space, making it ideal for applications like recommendation systems and natural language processing.\n",
      "Judge result: {'score': 5, 'label': 'excellent', 'reason': 'The model response accurately describes what a vector database does, mentioning both the storage of high-dimensional vectors and the efficient similarity search, aligning well with the reference answer.'}\n",
      "\n",
      "=== Evaluating item 2 ===\n",
      "Prompt: List three benefits of using LLM-as-a-judge for model evaluation.\n",
      "Candidate response:\n",
      " Using LLMs (Large Language Models) as judges for model evaluation offers several benefits:\n",
      "\n",
      "1. **Consistency and Objectivity**: LLMs can provide a consistent evaluation framework that is free from human biases and variability. They can apply the same criteria uniformly across different models and datasets, ensuring that evaluations are based on the same standards every time.\n",
      "\n",
      "2. **Scalability**: LLMs can evaluate a large number of models and outputs quickly and efficiently. This scalability allows for comprehensive assessments across various tasks and datasets without the need for extensive human resources, making it feasible to evaluate many models in a shorter time frame.\n",
      "\n",
      "3. **Rich Feedback and Insights**: LLMs can generate detailed feedback and insights about model performance, including strengths and weaknesses in specific areas. This can help developers understand where their models excel and where improvements are needed, facilitating a more informed iterative development process.\n",
      "\n",
      "These benefits make LLMs a valuable tool in the model evaluation process, enhancing both the efficiency and quality of assessments.\n",
      "Judge result: {'score': 5, 'label': 'excellent', 'reason': 'The response clearly and accurately lists three benefits of using LLMs as judges for model evaluation: consistency and objectivity, scalability, and rich feedback and insights. Each point is well-explained and relevant to the prompt, making the answer complete and correct.'}\n",
      "\n",
      "=== Evaluating item 3 ===\n",
      "Prompt: Convert the sentence 'The model failed three times' into passive voice.\n",
      "Candidate response:\n",
      " The sentence in passive voice would be: \"Three times, the model was failed.\"\n",
      "Judge result: {'score': 2, 'label': 'fair', 'reason': 'The model response attempts to convert the sentence into passive voice but incorrectly implies that the model was actively failed by something else, which changes the meaning. The structure is also awkward and not a direct passive transformation of the original sentence.'}\n",
      "\n",
      "=== Evaluating item 4 ===\n",
      "Prompt: Summarize the following in one sentence: 'Reinforcement learning agents learn actions through rewards and penalties and improve performance over time.'\n",
      "Candidate response:\n",
      " Reinforcement learning agents enhance their performance by learning to take actions based on rewards and penalties.\n",
      "Judge result: {'score': 4, 'label': 'good', 'reason': \"The model response captures the essence of reinforcement learning by mentioning performance enhancement through actions based on rewards and penalties, but it slightly lacks the explicit mention of 'optimizing decisions' as in the reference answer.\"}\n",
      "\n",
      "=== Evaluating item 5 ===\n",
      "Prompt: Give me 2 ways to reduce hallucinations in a RAG system.\n",
      "Candidate response:\n",
      " Reducing hallucinations in a Retrieval-Augmented Generation (RAG) system can be approached in several ways. Here are two effective strategies:\n",
      "\n",
      "1. **Improved Retrieval Mechanism**:\n",
      "   - Enhance the retrieval component of the RAG system to ensure that it pulls in more relevant and accurate documents. This can be achieved by:\n",
      "     - **Using a more sophisticated retrieval model**: Implement advanced techniques such as dense retrieval (e.g., using embeddings from models like BERT or Sentence Transformers) to better match queries with relevant documents.\n",
      "     - **Incorporating relevance feedback**: Use user feedback or reinforcement learning to continuously improve the retrieval process based on what documents lead to more accurate and relevant responses.\n",
      "\n",
      "2. **Fine-tuning the Generation Model**:\n",
      "   - Fine-tune the generative model on a dataset that emphasizes factual accuracy and coherence. This can involve:\n",
      "     - **Training on high-quality, domain-specific datasets**: Use curated datasets that contain accurate information and are representative of the types of queries the system will handle.\n",
      "     - **Implementing constraints during generation**: Use techniques like controlled generation or prompt engineering to guide the model towards producing more factual and relevant outputs, reducing the likelihood of generating hallucinated information.\n",
      "\n",
      "By focusing on these two areas, you can significantly mitigate the occurrence of hallucinations in a RAG system.\n",
      "Judge result: {'score': 4, 'label': 'good', 'reason': 'The model response provides two effective strategies for reducing hallucinations in a RAG system: improving the retrieval mechanism and fine-tuning the generation model. These strategies are well-explained and relevant. However, the response could be improved by explicitly mentioning the importance of using retrieved documents as a basis for generation, aligning more closely with the reference answer.'}\n",
      "\n",
      "=== Evaluating item 6 ===\n",
      "Prompt: Is this statement true or false? 'FAISS is a text generation model.' Justify your answer.\n",
      "Candidate response:\n",
      " The statement is false. FAISS (Facebook AI Similarity Search) is not a text generation model; rather, it is a library developed by Facebook AI Research for efficient similarity search and clustering of dense vectors. FAISS is primarily used for tasks such as nearest neighbor search in high-dimensional spaces, which is often applied in scenarios like searching for similar items in large datasets, including images, text embeddings, and more.\n",
      "\n",
      "Text generation models, on the other hand, are designed to generate human-like text based on input prompts. Examples of text generation models include GPT (Generative Pre-trained Transformer) and other language models.\n",
      "\n",
      "In summary, FAISS is focused on similarity search and vector operations, not on generating text.\n",
      "Judge result: {'score': 5, 'label': 'excellent', 'reason': 'The model response is accurate, complete, and well-explained. It correctly identifies FAISS as a library for similarity search and clustering of dense vectors, not a text generation model, and provides a clear distinction between FAISS and text generation models.'}\n",
      "\n",
      "=== Evaluating item 7 ===\n",
      "Prompt: Write a single SQL query to fetch all employees earning more than 50,000.\n",
      "Candidate response:\n",
      " To fetch all employees earning more than 50,000, you can use the following SQL query. This assumes you have a table named `employees` with a column named `salary` that stores the salary of each employee.\n",
      "\n",
      "```sql\n",
      "SELECT *\n",
      "FROM employees\n",
      "WHERE salary > 50000;\n",
      "```\n",
      "\n",
      "This query selects all columns (`*`) from the `employees` table where the `salary` is greater than 50,000. Make sure to adjust the table and column names if they differ in your database schema.\n",
      "Judge result: {'score': 5, 'label': 'excellent', 'reason': 'The model response provides a correct and complete SQL query that matches the reference answer. It also includes a clear explanation of the query, assuming a typical database schema, and advises on adjusting table and column names if necessary.'}\n",
      "\n",
      "=== Evaluating item 8 ===\n",
      "Prompt: Provide a polite refusal to this request: 'Hack into my colleagueâ€™s email.'\n",
      "Candidate response:\n",
      " Subject: Re: Request for Assistance\n",
      "\n",
      "Dear [Name],\n",
      "\n",
      "Thank you for reaching out. I appreciate your trust in me, but I must respectfully decline your request to hack into your colleagueâ€™s email. Such actions are not only unethical but also illegal, and I believe itâ€™s important to maintain integrity and respect for othersâ€™ privacy.\n",
      "\n",
      "If thereâ€™s a specific issue youâ€™re facing with your colleague, I would be happy to help you find a more appropriate and constructive way to address it.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "Judge result: {'score': 5, 'label': 'excellent', 'reason': 'The response is polite, clear, and provides a strong ethical and legal rationale for refusing the request. It also offers an alternative solution, which aligns well with the instruction to provide a polite refusal.'}\n",
      "\n",
      "=== Evaluating item 9 ===\n",
      "Prompt: Explain 'prompt chaining' in less than 25 words.\n",
      "Candidate response:\n",
      " Prompt chaining is a technique where outputs from one prompt are used as inputs for subsequent prompts to create a coherent and extended interaction.\n",
      "Judge result: {'score': 5, 'label': 'excellent', 'reason': 'The response accurately and concisely explains prompt chaining, aligning well with the reference answer and staying within the word limit.'}\n",
      "\n",
      "=== Evaluating item 10 ===\n",
      "Prompt: Suggest three test metrics to evaluate a generative AI model.\n",
      "Candidate response:\n",
      " Evaluating a generative AI model involves assessing various aspects of its performance, including quality, diversity, and coherence of the generated outputs. Here are three test metrics that can be used to evaluate a generative AI model:\n",
      "\n",
      "1. **Inception Score (IS)**:\n",
      "   - The Inception Score measures the quality and diversity of generated images. It uses a pre-trained Inception model to classify the generated images and evaluates how well the model can distinguish between different classes. A higher Inception Score indicates that the generated images are both high-quality and diverse, as it reflects the model's ability to produce images that are recognizable and varied.\n",
      "\n",
      "2. **FrÃ©chet Inception Distance (FID)**:\n",
      "   - FID is another metric used primarily for evaluating the quality of generated images. It compares the distribution of generated images to the distribution of real images in the feature space of a pre-trained Inception model. A lower FID score indicates that the generated images are closer to the real images in terms of their statistical properties, suggesting higher quality and realism.\n",
      "\n",
      "3. **BLEU Score (for text generation)**:\n",
      "   - The BLEU (Bilingual Evaluation Understudy) score is commonly used to evaluate the quality of text generated by models, particularly in tasks like machine translation and text summarization. It measures the overlap between n-grams in the generated text and reference texts. A higher BLEU score indicates that the generated text is more similar to the reference texts, reflecting better quality and relevance.\n",
      "\n",
      "These metrics can be adapted or combined depending on the specific type of generative AI model (e.g., image generation, text generation) and the goals of the evaluation.\n",
      "Judge result: {'score': 5, 'label': 'excellent', 'reason': 'The response provides a comprehensive and accurate explanation of three relevant metrics for evaluating generative AI models: Inception Score, FrÃ©chet Inception Distance, and BLEU Score. It clearly explains the purpose and application of each metric, covering both image and text generation, which demonstrates a thorough understanding of the evaluation process.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results: List[Dict] = []\n",
    "\n",
    "for item in dataset:\n",
    "    prompt = item[\"prompt\"]\n",
    "    reference = item.get(\"reference\")\n",
    "\n",
    "    print(f\"\\n=== Evaluating item {item['id']} ===\")\n",
    "    print(\"Prompt:\", prompt)\n",
    "\n",
    "    # 1) Generate candidate answer from target model\n",
    "    candidate = generate_candidate(\n",
    "        target_model_id=TARGET_MODEL_ID,\n",
    "        instruction=prompt,\n",
    "    )\n",
    "    print(\"Candidate response:\\n\", candidate)\n",
    "\n",
    "    # 2) Judge the response\n",
    "    grade = judge_response(\n",
    "        judge_model_id=JUDGE_MODEL_ID,\n",
    "        instruction=prompt,\n",
    "        model_response=candidate,\n",
    "        reference_answer=reference,\n",
    "    )\n",
    "\n",
    "    print(\"Judge result:\", grade)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"prompt\": prompt,\n",
    "            \"candidate\": candidate,\n",
    "            \"score\": grade[\"score\"],\n",
    "            \"label\": grade.get(\"label\"),\n",
    "            \"reason\": grade[\"reason\"],\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9859a-c0ed-4a62-a6eb-01e24ae964ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "219ddbc2-617a-4ad5-911a-62be4c97b208",
   "metadata": {},
   "source": [
    "### 9. Inspect results in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3af31540-8db1-4d55-9c93-288bb90c7bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "ID 1: score=5 label=excellent\n",
      "  Reason: The model response accurately describes what a vector database does, mentioning both the storage of high-dimensional vectors and the efficient similarity search, aligning well with the reference answer.\n",
      "ID 2: score=5 label=excellent\n",
      "  Reason: The response clearly and accurately lists three benefits of using LLMs as judges for model evaluation: consistency and objectivity, scalability, and rich feedback and insights. Each point is well-explained and relevant to the prompt, making the answer complete and correct.\n",
      "ID 3: score=2 label=fair\n",
      "  Reason: The model response attempts to convert the sentence into passive voice but incorrectly implies that the model was actively failed by something else, which changes the meaning. The structure is also awkward and not a direct passive transformation of the original sentence.\n",
      "ID 4: score=4 label=good\n",
      "  Reason: The model response captures the essence of reinforcement learning by mentioning performance enhancement through actions based on rewards and penalties, but it slightly lacks the explicit mention of 'optimizing decisions' as in the reference answer.\n",
      "ID 5: score=4 label=good\n",
      "  Reason: The model response provides two effective strategies for reducing hallucinations in a RAG system: improving the retrieval mechanism and fine-tuning the generation model. These strategies are well-explained and relevant. However, the response could be improved by explicitly mentioning the importance of using retrieved documents as a basis for generation, aligning more closely with the reference answer.\n",
      "ID 6: score=5 label=excellent\n",
      "  Reason: The model response is accurate, complete, and well-explained. It correctly identifies FAISS as a library for similarity search and clustering of dense vectors, not a text generation model, and provides a clear distinction between FAISS and text generation models.\n",
      "ID 7: score=5 label=excellent\n",
      "  Reason: The model response provides a correct and complete SQL query that matches the reference answer. It also includes a clear explanation of the query, assuming a typical database schema, and advises on adjusting table and column names if necessary.\n",
      "ID 8: score=5 label=excellent\n",
      "  Reason: The response is polite, clear, and provides a strong ethical and legal rationale for refusing the request. It also offers an alternative solution, which aligns well with the instruction to provide a polite refusal.\n",
      "ID 9: score=5 label=excellent\n",
      "  Reason: The response accurately and concisely explains prompt chaining, aligning well with the reference answer and staying within the word limit.\n",
      "ID 10: score=5 label=excellent\n",
      "  Reason: The response provides a comprehensive and accurate explanation of three relevant metrics for evaluating generative AI models: Inception Score, FrÃ©chet Inception Distance, and BLEU Score. It clearly explains the purpose and application of each metric, covering both image and text generation, which demonstrates a thorough understanding of the evaluation process.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "for r in results:\n",
    "    print(f\"ID {r['id']}: score={r['score']} label={r['label']}\")\n",
    "    print(\"  Reason:\", r[\"reason\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860740e-1ca3-47f3-ab60-f0265b664d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15284f19-a645-4f20-b33b-4bf291a031c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
