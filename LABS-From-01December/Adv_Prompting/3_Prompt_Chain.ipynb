{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bd3565",
   "metadata": {},
   "source": [
    "# 3. Prompt Chaining with OpenAI Responses API\n",
    "\n",
    "Prompt Chaining is a technique in LLM workflows where you break a complex task into multiple smaller steps, each handled by its own prompt.\n",
    "The output of one prompt becomes the input to the next, forming a chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66b2c6-48b0-4def-b27d-aefe5e099517",
   "metadata": {},
   "source": [
    "#### ðŸ§  Why use Prompt Chaining?\n",
    "\n",
    "LLMs struggle with long, multi-step reasoning in a single prompt.\n",
    "By chaining prompts, you:\n",
    "\n",
    "| Benefit              | Meaning                              |\n",
    "| -------------------- | ------------------------------------ |\n",
    "| Decompose complexity | Handle one step at a time            |\n",
    "| Improve accuracy     | Fewer hallucinations                 |\n",
    "| Add structure        | Each step has a clear responsibility |\n",
    "| Enable modularity    | Easy to replace or improve one step  |\n",
    "\n",
    "Prompt 1 â†’ Output A â†’ Prompt 2 â†’ Output B â†’ Prompt 3 â†’ Final Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6e253-36dd-4c18-8a34-d62cccaaf2cb",
   "metadata": {},
   "source": [
    "This notebook shows how to build **stepâ€‘byâ€‘step prompt chains** using the modern `client.responses.create()` API.\n",
    "We will:\n",
    "- Configure the OpenAI client and model\n",
    "- Send a single prompt\n",
    "- Build a **twoâ€‘step chain** (outline â†’ expand)\n",
    "- Build a **multiâ€‘step chain** with a small helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0286ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 0 â€” Imports & client setup\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f613f152-ee96-44d6-b9d0-444826bd7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load API key from environment variable OPENAI_API_KEY\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # make sure this is set in your environment\n",
    ")\n",
    "\n",
    "# Global model config â€” change here to switch models everywhere\n",
    "model_name = \"gpt-4o-mini\"  # e.g. \"gpt-4o\", \"gpt-4.1-mini\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29312f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Helper: small wrapper so we don't repeat boilerplate\n",
    "def run_llm(user_message: str, system_message: str | None = None):\n",
    "    \"\"\"Send a simple 1â€‘turn request and return output_text.\"\"\"\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model_name,\n",
    "        input=messages,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16abf7",
   "metadata": {},
   "source": [
    "## Step 1 â€” Single prompt\n",
    "\n",
    "Start with a single question to the model using `run_llm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d030bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Definition**: Prompt chaining involves using the output of one prompt as the input for the next, creating a sequence that builds on previous responses to generate more complex or refined results.\n",
      "\n",
      "- **Purpose**: This technique enhances the depth and coherence of generated content, allowing for more nuanced and contextually aware interactions, especially in tasks requiring multi-step reasoning.\n",
      "\n",
      "- **Application**: Commonly used in creative writing, problem-solving, and programming tasks, prompt chaining helps in developing narratives, solving complex queries, or iterating on ideas effectively.\n"
     ]
    }
   ],
   "source": [
    "single_answer = run_llm(\n",
    "    user_message=\"Explain prompt chaining in 3 bullet points.\",\n",
    "    system_message=\"You are a concise AI tutor.\",\n",
    ")\n",
    "print(single_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f10dac",
   "metadata": {},
   "source": [
    "## Step 2 â€” Twoâ€‘step prompt chain (outline â†’ expand)\n",
    "\n",
    "We first ask the model to generate an **outline**, then feed that outline back in a second call\n",
    "to generate a detailed explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b949bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- OUTLINE ---\n",
      " ### Outline for a 5-Minute Talk on 'Prompt Chaining for Beginners'\n",
      "\n",
      "1. **Introduction to Prompt Chaining**\n",
      "   - Define prompt chaining and its significance in AI interactions.\n",
      "   - Briefly explain how it enhances the effectiveness of AI responses.\n",
      "\n",
      "2. **Basic Concepts of Prompt Chaining**\n",
      "   - Explain the concept of prompts and how they guide AI.\n",
      "   - Introduce the idea of linking prompts to build context.\n",
      "\n",
      "3. **Step-by-Step Process**\n",
      "   - Demonstrate how to create a simple prompt chain.\n",
      "   - Provide an example of a basic prompt followed by a follow-up prompt.\n",
      "\n",
      "4. **Best Practices for Effective Prompt Chaining**\n",
      "   - Tips for clarity and specificity in prompts.\n",
      "   - Importance of iterative refinement to improve results.\n",
      "\n",
      "5. **Conclusion and Q&A**\n",
      "   - Recap the benefits of prompt chaining.\n",
      "   - Invite questions and encourage experimentation with prompt chaining in AI applications.\n"
     ]
    }
   ],
   "source": [
    "# Step 2a â€” Ask for an outline\n",
    "outline = run_llm(\n",
    "    user_message=(\n",
    "        \"Create a short outline for a 5â€‘minute talk on 'Prompt Chaining for Beginners'. \"\n",
    "        \"Use 4â€“5 bullet points.\"\n",
    "    ),\n",
    "    system_message=\"You are a helpful tech speaker coach.\",\n",
    ")\n",
    "print(\"--- OUTLINE ---\\n\", outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744fea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EXPANDED SCRIPT ---\n",
      " ### Script for a 5-Minute Talk on 'Prompt Chaining for Beginners'\n",
      "\n",
      "**Introduction to Prompt Chaining**\n",
      "\n",
      "Hello everyone! Today, weâ€™re going to talk about something called prompt chaining. So, what is prompt chaining? Simply put, itâ€™s a way to connect different prompts when youâ€™re interacting with AI. This technique helps the AI understand your questions better and gives you more useful answers. By linking prompts, we can create a clearer context for the AI, making our conversations smoother and more effective.\n",
      "\n",
      "**Basic Concepts of Prompt Chaining**\n",
      "\n",
      "Letâ€™s break this down a bit. First, what is a prompt? A prompt is basically a question or instruction you give to the AI to get a response. Think of it as a way to guide the AI in the direction you want it to go. Now, when we talk about prompt chaining, we mean linking these prompts together. This helps build context, so the AI can follow your train of thought and give more relevant answers.\n",
      "\n",
      "**Step-by-Step Process**\n",
      "\n",
      "Now, letâ€™s see how to create a simple prompt chain. Hereâ€™s a basic example:\n",
      "\n",
      "1. **First Prompt:** â€œWhat are the benefits of exercise?â€\n",
      "   \n",
      "   The AI will respond with some great points about how exercise is good for health, mood, and energy.\n",
      "\n",
      "2. **Follow-Up Prompt:** â€œCan you explain how exercise affects mental health?â€\n",
      "\n",
      "   By asking this follow-up, youâ€™re giving the AI more context. It now knows youâ€™re interested in the mental health aspect, so it can provide a more focused answer.\n",
      "\n",
      "This is how prompt chaining works! You start with a general question and then narrow it down with follow-up prompts.\n",
      "\n",
      "**Best Practices for Effective Prompt Chaining**\n",
      "\n",
      "Now, letâ€™s talk about some best practices. First, be clear and specific in your prompts. The more precise you are, the better the AI can understand what you want. For example, instead of asking, â€œTell me about dogs,â€ you might say, â€œWhat are the different breeds of dogs and their characteristics?â€ This gives the AI a clearer direction.\n",
      "\n",
      "Second, donâ€™t hesitate to refine your prompts. If the AI doesnâ€™t give you the answer you were hoping for, try rephrasing your question or adding more detail. This iterative process helps improve the results you get.\n",
      "\n",
      "**Conclusion and Q&A**\n",
      "\n",
      "In conclusion, prompt chaining is a powerful tool that can enhance your interactions with AI. By linking prompts together, you can create a more coherent conversation and get better answers. I encourage you to experiment with this technique in your own AI applications.\n",
      "\n",
      "Now, Iâ€™d love to hear your questions! Feel free to ask anything about prompt chaining or how to use it effectively. Thank you!\n"
     ]
    }
   ],
   "source": [
    "# Step 2b â€” Use the outline as input to expand into a mini talk script\n",
    "expanded_script = run_llm(\n",
    "    user_message=(\n",
    "        \"You are given this outline for a 5â€‘minute talk on prompt chaining:\\n\\n\"\n",
    "        f\"{outline}\\n\\n\"\n",
    "        \"Turn it into a short script in simple English. Keep it under 500 words.\"\n",
    "    ),\n",
    "    system_message=\"You write clear, friendly technical explanations.\",\n",
    ")\n",
    "print(\"--- EXPANDED SCRIPT ---\\n\", expanded_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab13aa-7df6-437a-ba6b-970baa165e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c1599-7ce9-4ffa-bebb-bc2500c7b85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87f7a7-5b39-4928-b3ab-13d31a02c274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c0fb05d",
   "metadata": {},
   "source": [
    "## Step 3 â€” Generalising prompt chaining\n",
    "\n",
    "Let's build a small **prompt chain helper** that:\n",
    "- Takes an initial user query\n",
    "- Runs a series of *steps* (functions) that each call the model\n",
    "- Passes intermediate results forward\n",
    "\n",
    "We will implement a 3â€‘step chain:\n",
    "1. Rewrite the original question more clearly\n",
    "2. Break it into subâ€‘questions\n",
    "3. Answer all subâ€‘questions and summarise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4800a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_rewrite_question(original_question: str) -> str:\n",
    "    return run_llm(\n",
    "        user_message=(\n",
    "            \"Rewrite this question to be clearer and more precise. \"\n",
    "            \"Keep the *same intent*:\\n\\n\" + original_question\n",
    "        ),\n",
    "        system_message=\"You are an expert at clarifying user questions.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def step_decompose_question(clear_question: str) -> str:\n",
    "    return run_llm(\n",
    "        user_message=(\n",
    "            \"Break this question into 3â€“5 smaller subâ€‘questions that, when answered, \"\n",
    "            \"fully address the original question. Number the subâ€‘questions.\\n\\n\"\n",
    "            + clear_question\n",
    "        ),\n",
    "        system_message=\"You are good at problem decomposition.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def step_answer_subquestions(clear_question: str, sub_questions: str) -> str:\n",
    "    return run_llm(\n",
    "        user_message=(\n",
    "            \"You are given an original question and a set of numbered subâ€‘questions.\\n\\n\"\n",
    "            f\"Original question: {clear_question}\\n\\n\"\n",
    "            f\"Subâ€‘questions: \\n{sub_questions}\\n\\n\"\n",
    "            \"Answer each subâ€‘question briefly, then provide a 1â€‘paragraph overall answer.\"\n",
    "        ),\n",
    "        system_message=\"You are an expert tutor who answers stepâ€‘byâ€‘step.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253a75e-2ccb-4cbb-b20c-1d15aba9a5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29efca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together: a generic chaining function\n",
    "def run_prompt_chain(original_question: str) -> dict:\n",
    "    \"\"\"Run a 3â€‘step chain and return all intermediate steps.\"\"\"\n",
    "    clear_q = step_rewrite_question(original_question)\n",
    "    subs = step_decompose_question(clear_q)\n",
    "    final_answer = step_answer_subquestions(clear_q, subs)\n",
    "\n",
    "    return {\n",
    "        \"original_question\": original_question,\n",
    "        \"clear_question\": clear_q,\n",
    "        \"sub_questions\": subs,\n",
    "        \"final_answer\": final_answer,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27fd96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== original_question =====\n",
      "How can I use prompt chaining to improve my RAG system's answers?\n",
      "\n",
      "===== clear_question =====\n",
      "How can I implement prompt chaining to enhance the responses of my RAG system?\n",
      "\n",
      "===== sub_questions =====\n",
      "To address the question of implementing prompt chaining to enhance the responses of your RAG (Retrieval-Augmented Generation) system, consider the following sub-questions:\n",
      "\n",
      "1. **What is prompt chaining, and how does it work in the context of RAG systems?**\n",
      "2. **What are the specific goals or improvements I want to achieve by using prompt chaining in my RAG system?**\n",
      "3. **What types of prompts should I design for each stage of the chaining process to maximize the quality of responses?**\n",
      "4. **How can I evaluate the effectiveness of the prompt chaining implementation on the responses generated by my RAG system?**\n",
      "5. **What tools or frameworks can assist in the implementation and testing of prompt chaining within my RAG system?**\n",
      "\n",
      "===== final_answer =====\n",
      "### 1. What is prompt chaining, and how does it work in the context of RAG systems?\n",
      "Prompt chaining is a technique where multiple prompts are linked together in a sequence to refine and enhance the output of a model. In RAG systems, this involves using the output from one prompt as input for the next, allowing for iterative refinement of responses. This can help in generating more coherent and contextually relevant answers by building on previous outputs.\n",
      "\n",
      "### 2. What are the specific goals or improvements I want to achieve by using prompt chaining in my RAG system?\n",
      "The specific goals may include improving the accuracy of responses, enhancing contextual understanding, reducing ambiguity, and increasing the relevance of information retrieved. Additionally, you may aim to create more engaging and informative interactions by guiding the model through a structured thought process.\n",
      "\n",
      "### 3. What types of prompts should I design for each stage of the chaining process to maximize the quality of responses?\n",
      "Prompts should be designed to serve different purposes at each stage. For example, the first prompt could focus on gathering initial information, the second could ask for clarification or elaboration, and the third could request a summary or synthesis of the information. Tailoring prompts to guide the model through a logical progression will enhance the quality of the final output.\n",
      "\n",
      "### 4. How can I evaluate the effectiveness of the prompt chaining implementation on the responses generated by my RAG system?\n",
      "Effectiveness can be evaluated through qualitative and quantitative metrics. You can conduct user studies to gather feedback on response quality, coherence, and relevance. Additionally, you can use metrics like BLEU or ROUGE scores to compare generated responses against a set of reference answers, as well as track user engagement and satisfaction over time.\n",
      "\n",
      "### 5. What tools or frameworks can assist in the implementation and testing of prompt chaining within my RAG system?\n",
      "Several tools and frameworks can assist, including Hugging Face's Transformers for model integration, LangChain for managing prompt workflows, and various A/B testing frameworks for evaluating different prompt strategies. Additionally, platforms like OpenAIâ€™s API can facilitate rapid prototyping and testing of chained prompts.\n",
      "\n",
      "### Overall Answer\n",
      "To implement prompt chaining in your RAG system, start by understanding that it involves linking multiple prompts to iteratively refine model outputs. Define clear goals, such as improving accuracy and relevance, to guide your design process. Create tailored prompts for each stage of the chain, focusing on gathering information, clarifying responses, and synthesizing outputs. Evaluate effectiveness through user feedback and established metrics, and leverage tools like Hugging Face and LangChain to streamline implementation and testing. This structured approach will enhance the quality and coherence of responses generated by your RAG system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try the chain\n",
    "question = \"How can I use prompt chaining to improve my RAG system's answers?\"\n",
    "chain_result = run_prompt_chain(question)\n",
    "\n",
    "for key, value in chain_result.items():\n",
    "    print(\"=====\", key, \"=====\")\n",
    "    print(value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b60904-9603-4548-ab9d-aebf6c803021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
