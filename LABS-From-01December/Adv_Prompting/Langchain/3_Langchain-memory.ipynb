{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080a37ab",
   "metadata": {},
   "source": [
    "## Langchain  - memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833a901f",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f234de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4a7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your OpenAI API key from a .env file\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d1bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model (using gpt-4o-mini)\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93890824",
   "metadata": {},
   "source": [
    "* Conversational Memory\n",
    "\n",
    "This function creates in-memory chat history (new chat history) for each unique session ID or returns an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4298ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in-memory chat history store\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443db82-d6ce-44d9-9211-00db46750cb8",
   "metadata": {},
   "source": [
    "get_session_history() retrieves the chat history that belongs to a particular session (e.g., a user's conversation). If the session does not already have a history stored, it creates a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69b662-01b8-4be5-abcd-a9da2a391623",
   "metadata": {},
   "source": [
    "#### Different type message history implementation\n",
    "**In-memory implementations\n",
    "üóÑÔ∏èInMemoryChatMessageHistory**\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "- Stores messages in Python memory\n",
    "- Lost when the program stops\n",
    "- Fast, no external dependencies\n",
    "- Best for prototyping\n",
    "\n",
    "\n",
    "**üíæ File-based implementations\n",
    " FileChatMessageHistory**\n",
    " \n",
    "from langchain_core.chat_history import FileChatMessageHistory\n",
    "\n",
    "- Stores messages in a local file (usually JSON)\n",
    "- Persisted across runs\n",
    "- Good for single-user apps or desktop tools\n",
    "\n",
    "**üõ¢Ô∏è Database-backed implementations\n",
    " SQLChatMessageHistory**\n",
    " \n",
    "from langchain_core.chat_history import SQLChatMessageHistory\n",
    "- Saves messages in SQLite / Postgres\n",
    "- Suitable for production apps\n",
    "- Supports multiple users and scaling\n",
    "\n",
    "**RedisChatMessageHistory**\n",
    "\n",
    "from langchain_core.chat_history import RedisChatMessageHistory\n",
    "- Stores chat history in Redis\n",
    "- Allows TTL (auto-expire messages)\n",
    "- Best for web-scale and session-based chat\n",
    "\n",
    "**‚òÅÔ∏è Cloud-native / external store\n",
    "\n",
    " MongoDBChatMessageHistory**\n",
    "- Stores messages in MongoDB\n",
    "- Flexible document-based structure\n",
    "\n",
    "**DynamoDBChatMessageHistory\n",
    "AWS DynamoDB storage**\n",
    "- Fully managed serverless persistence\n",
    "\n",
    "**üß≠ Specialized histories\n",
    " BullMQChatMessageHistory**\n",
    " \n",
    "- For Node-based queue-backed workflows\n",
    "\n",
    "**ChatMessageHistoryWithSummary**\n",
    "- Wraps another history store\n",
    "- Adds summarization to prevent context overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384c2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a prompt template for the chatbot\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed279ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conversation with history handling\n",
    "session_id = \"abc123\"  # Unique session identifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75bcd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine the model with the prompt and message history\n",
    "chain = prompt | model\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "# RunnableWithMessageHistory is a LangChain wrapper that adds chat history capability to any runnable chain.\n",
    "# RunnableWithMessageHistory is agnostic as to how the get_session_history callable retrieves its chat message histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba0ec4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Can you tell me when MS Dhoni will retire?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: As of my last update in October 2023, MS Dhoni has not publicly announced a specific date for his retirement from cricket. He retired from international cricket on August 15, 2020, but continues to play in franchise cricket, including the Indian Premier League (IPL) for the Chennai Super Kings. Any news regarding his future retirement would need to be checked from the latest sources or updates.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I mean from IPL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: As of my last knowledge update in October 2023, MS Dhoni had not officially announced a retirement date from the Indian Premier League (IPL). He continued to play for the Chennai Super Kings in the IPL. For the latest updates on his retirement or participation in future seasons, it's best to check recent news sources or official announcements from the IPL or Chennai Super Kings.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start a conversation\n",
    "print(\"Chatbot: Hi! How can I assist you today?\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Exit condition\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Stream response from the model\n",
    "    print(\"Chatbot: \", end=\"\", flush=True)\n",
    "    for r in with_message_history.stream(\n",
    "            {\"messages\": [HumanMessage(content=user_input)]},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "    ):\n",
    "        print(r.content, end=\"\", flush=True)\n",
    "    print()  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb17c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132518d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
